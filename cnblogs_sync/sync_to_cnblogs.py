# cnblogs_sync/sync_to_cnblogs.py
#
# åšå®¢å›­æ–‡ç« å‘å¸ƒè„šæœ¬
#
# ã€åŠŸèƒ½è¯´æ˜ã€‘
# å°†æœ¬åœ° Markdown æ–‡ä»¶å‘å¸ƒåˆ°åšå®¢å›­ï¼ˆå•å‘ï¼šæœ¬åœ° â†’ åšå®¢å›­ï¼‰ï¼Œæ”¯æŒåŸºäºæœ¬åœ° JSON è®°å½•çš„å»é‡åˆ¤æ–­ã€‚
#
# ã€ç¯å¢ƒå˜é‡é…ç½®ã€‘
# ä½¿ç”¨å‰éœ€è¦è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼ˆé€šè¿‡ .env æ–‡ä»¶æˆ–ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼‰ï¼š
#   - CNBLOGS_RPC_URL: åšå®¢å›­ RPC åœ°å€ï¼ˆå¿…éœ€ï¼‰
#   - CNBLOGS_USERNAME: ç”¨æˆ·åï¼ˆå¿…éœ€ï¼‰
#   - CNBLOGS_TOKEN: Tokenï¼ˆå¿…éœ€ï¼‰
#   - SYNC_REPO_URL: ç›®æ ‡ä»“åº“åœ°å€ï¼ˆå¿…éœ€ï¼‰
#   - SYNC_REPO_TOKEN: æ¨é€çŠ¶æ€åˆ†æ”¯ç”¨çš„ Tokenï¼ˆå¿…éœ€ï¼‰
#
# ã€ä½¿ç”¨æ–¹æ³•ã€‘
#
# 1. å‘å¸ƒæ–‡ç« åˆ°åšå®¢å›­ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨åˆå§‹åŒ–å‘å¸ƒè®°å½•ï¼‰ï¼š
#    a) è‡ªåŠ¨æ¨¡å¼ï¼ˆæ¨èï¼‰ï¼šä¸æŒ‡å®šæ–‡ä»¶ï¼Œè‡ªåŠ¨æ‰«æä»“åº“ä¸­æ‰€æœ‰ .md æ–‡ä»¶
#       python cnblogs_sync/sync_to_cnblogs.py
#    b) æ‰‹åŠ¨æ¨¡å¼ï¼šæŒ‡å®šè¦å‘å¸ƒçš„æ–‡ä»¶
#       python cnblogs_sync/sync_to_cnblogs.py <file1.md> [file2.md] ...
#    è¯´æ˜ï¼šå°† Markdown æ–‡ä»¶å‘å¸ƒåˆ°åšå®¢å›­
#          - è‹¥å‘å¸ƒè®°å½•ä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨ä» API è·å–æœ€è¿‘ 300 ç¯‡æ–‡ç« ç”Ÿæˆè®°å½•
#          - å¦‚æœæ–‡ç« å·²åœ¨æœ¬åœ°è®°å½•ä¸­ï¼ˆå·²å‘å¸ƒè¿‡ï¼‰ï¼Œé»˜è®¤æ‰§è¡Œæ›´æ–°
#          - å¦‚æœæ˜¯æ–°æ–‡ç« ï¼Œç›´æ¥å‘å¸ƒå¹¶è‡ªåŠ¨æ›´æ–°æœ¬åœ°è®°å½•
#          - è‡ªåŠ¨æ¨¡å¼ä¼šæ’é™¤ .gitã€.githubã€node_modulesã€cnblogs_sync ç­‰ç›®å½•
#
# ã€æœ¬åœ°è®°å½•æ–‡ä»¶ã€‘
# - ä½ç½®ï¼šé»˜è®¤åœ¨ä»“åº“å†…çš„ `.cnblogs_sync/.cnblogs_sync_record.json`
# - æ ¼å¼ï¼š{ "æ–‡ç« æ ‡é¢˜": "post_id", ... }
# - ä½œç”¨ï¼šè®°å½•å·²å‘å¸ƒåˆ°åšå®¢å›­çš„æ–‡ç« ï¼Œé¿å…é‡å¤å‘å¸ƒ

import os
import sys
import re
import json
import time
import subprocess
import tempfile
import shutil
import xmlrpc.client
from urllib.parse import urlparse, urlunparse, quote
from datetime import datetime
from pathlib import Path
from typing import Literal
from dotenv import load_dotenv
# from urllib.parse import quote # ä¸å†éœ€è¦è¿™ä¸ªæ¨¡å—ï¼Œå¯ä»¥ç§»é™¤

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# --- é…ç½®ä¿¡æ¯ï¼ˆä»…ä¿ç•™å¿…éœ€é¡¹ï¼‰ ---
RPC_URL = os.getenv("CNBLOGS_RPC_URL")
USERNAME = os.getenv("CNBLOGS_USERNAME")
PASSWORD = os.getenv("CNBLOGS_TOKEN")

SYNC_REPO_URL = os.getenv("SYNC_REPO_URL")
SYNC_REPO_TOKEN = os.getenv("SYNC_REPO_TOKEN")

# ä¸‹é¢ä¸ºå›ºå®šé»˜è®¤å€¼ï¼Œä¸å¯¹å¤–æš´éœ²é…ç½®
BLOG_ID = None  # è‡ªåŠ¨è·å–
KNOWLEDGE_BASE_URL = "https://assemble.gitbook.io/assemble"
CNBLOGS_SEARCH_URL = "https://zzk.cnblogs.com/my/s/blogpost-p"

# --- Git / è¿è¡Œç¯å¢ƒå°ä¼˜åŒ– ---
# é¿å…åœ¨æ— äº¤äº’ç¯å¢ƒï¼ˆZeabur/Cronï¼‰é‡Œ git push è§¦å‘å‡­æ®äº¤äº’å¡æ­»
os.environ.setdefault("GIT_TERMINAL_PROMPT", "0")

# --- è¡Œä¸ºå¼€å…³ ---
FORCE_OVERWRITE_EXISTING = True

# --- ä»“åº“æ ¹ç›®å½•ï¼ˆæ”¯æŒå¤–éƒ¨ä¼ å…¥ï¼‰ ---
REPO_ROOT = Path.cwd().resolve()

# --- è®°å½•/çŠ¶æ€æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ç›¸å¯¹ä»“åº“æ ¹ç›®å½•ï¼‰ ---
SYNC_RECORD_PATH = ".cnblogs_sync/.cnblogs_sync_record.json"
SYNC_STATE_PATH = ".cnblogs_sync/state.json"
SYNC_RUN_LOG_PATH = ".cnblogs_sync/run_history.jsonl"

def resolve_repo_path(path_str):
    """å°†è·¯å¾„è§£æä¸ºä»“åº“å†…ç»å¯¹è·¯å¾„ï¼ˆæ”¯æŒç»å¯¹è·¯å¾„ï¼‰"""
    p = Path(path_str)
    return p if p.is_absolute() else (REPO_ROOT / p)

# --- æœ¬åœ°å‘å¸ƒè®°å½•æ–‡ä»¶è·¯å¾„ ---
SYNC_RECORD_FILE = resolve_repo_path(SYNC_RECORD_PATH)
# --- æœ¬åœ°çŠ¶æ€æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºå¢é‡åŒæ­¥ï¼‰ ---
SYNC_STATE_FILE = resolve_repo_path(SYNC_STATE_PATH)
# --- æœ¬åœ°è¿è¡Œè®°å½•æ–‡ä»¶è·¯å¾„ ---
SYNC_RUN_LOG_FILE = resolve_repo_path(SYNC_RUN_LOG_PATH)

# --- å¢é‡åŒæ­¥ä¸ Git æŒä¹…åŒ–é…ç½® ---
INCREMENTAL_SYNC = True
SYNC_STATE_GIT = True
SYNC_STATE_BRANCH = "sync-state"
SYNC_STATE_REMOTE = "origin"

def build_state_remote_url():
    """åŸºäº SYNC_REPO_URL + SYNC_REPO_TOKEN è‡ªåŠ¨æ‹¼æ¥å¯å†™è¿œç«¯"""
    if not SYNC_REPO_URL or not SYNC_REPO_TOKEN:
        return None

    try:
        parsed = urlparse(SYNC_REPO_URL)
    except Exception:
        return None

    if parsed.scheme not in {"http", "https"}:
        print("âš ï¸ ä»…æ”¯æŒ http/https å½¢å¼çš„ SYNC_REPO_URL ç”¨äºè‡ªåŠ¨æ‹¼æ¥ Token")
        return None

    # è‹¥å·²åŒ…å«å‡­æ®ï¼Œç›´æ¥å¤ç”¨åŸå§‹ URL
    if parsed.username or parsed.password:
        return SYNC_REPO_URL

    token = quote(SYNC_REPO_TOKEN, safe="")
    netloc = f"{token}@{parsed.netloc}"
    return urlunparse(parsed._replace(netloc=netloc))

SYNC_STATE_REMOTE_URL = build_state_remote_url()

SYNC_STEPS = [
    "å‡†å¤‡ä¸æ¢å¤çŠ¶æ€",
    "åˆå§‹åŒ–å‘å¸ƒè®°å½•",
    "æ£€æµ‹å˜æ›´å¹¶ç”Ÿæˆå¾…å‘å¸ƒåˆ—è¡¨",
    "å‘å¸ƒ/æ›´æ–°æ–‡ç« ",
    "å†™å›çŠ¶æ€åˆ†æ”¯",
]


def log_plan():
    print("æ‰§è¡Œè®¡åˆ’ï¼ˆåŒæ­¥æµç¨‹ï¼‰ï¼š")
    for i, title in enumerate(SYNC_STEPS, 1):
        print(f"  {i}. {title}")


def log_step_start(step_index: int) -> None:
    print(f"\n[{step_index}/{len(SYNC_STEPS)}] {SYNC_STEPS[step_index - 1]}")


def log_step_ok(step_index: int, detail: str | None = None) -> None:
    title = SYNC_STEPS[step_index - 1]
    if detail:
        print(f"âœ… {title}ï¼š{detail}")
    else:
        print(f"âœ… {title} å®Œæˆ")


def log_step_skip(step_index: int, detail: str | None = None) -> None:
    title = SYNC_STEPS[step_index - 1]
    if detail:
        print(f"â­ï¸ {title}ï¼š{detail}")
    else:
        print(f"â­ï¸ {title} è·³è¿‡")


def log_step_fail(step_index: int, detail: str) -> None:
    title = SYNC_STEPS[step_index - 1]
    print(f"âŒ {title} å¤±è´¥ï¼š{detail}")

# --- éœ€è¦æ’é™¤çš„ç›®å½•ï¼ˆä¸æ‰«æè¿™äº›ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼‰ ---
EXCLUDE_DIRS = {'.git', '.github', 'node_modules', '__pycache__', '.vscode', '.idea', 'cnblogs_sync', '.cnblogs_sync'}

# --- å‡½æ•°å®šä¹‰ ---

def run_git(args, cwd=None, check=False):
    """è¿è¡Œ git å‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    cmd = ["git", "-c", "core.quotepath=false"] + args
    result = subprocess.run(
        cmd,
        cwd=cwd or REPO_ROOT,
        text=True,
        capture_output=True
    )
    if check and result.returncode != 0:
        raise RuntimeError(f"git {' '.join(args)} failed: {result.stderr.strip()}")
    return result

def is_git_repo():
    """æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦ä¸º Git ä»“åº“"""
    result = run_git(["rev-parse", "--is-inside-work-tree"])
    return result.returncode == 0 and result.stdout.strip() == "true"

def ensure_remote(remote, remote_url=None):
    """ç¡®ä¿ remote å­˜åœ¨å¹¶å¯ç”¨ï¼ˆå¯é€‰ï¼šè®¾ç½® PAT URLï¼‰"""
    if not remote_url:
        return
    result = run_git(["remote", "get-url", remote])
    if result.returncode != 0:
        run_git(["remote", "add", remote, remote_url], check=True)
        return
    current_url = result.stdout.strip()
    if current_url != remote_url:
        run_git(["remote", "set-url", remote, remote_url], check=True)

def has_remote(remote):
    """æ£€æŸ¥ remote æ˜¯å¦å­˜åœ¨"""
    result = run_git(["remote", "get-url", remote])
    return result.returncode == 0

def remote_branch_exists(remote, branch):
    """æ£€æŸ¥è¿œç«¯åˆ†æ”¯æ˜¯å¦å­˜åœ¨"""
    result = run_git(["ls-remote", "--heads", remote, branch])
    return result.returncode == 0 and bool(result.stdout.strip())

def ensure_git_identity(cwd):
    """ç¡®ä¿ git commit çš„ user.name/user.email å·²é…ç½®ï¼ˆCI/å®¹å™¨ç¯å¢ƒå¸¸ç¼ºçœï¼‰"""
    result_name = run_git(["config", "--get", "user.name"], cwd=cwd)
    result_email = run_git(["config", "--get", "user.email"], cwd=cwd)

    user_name = (result_name.stdout or "").strip() if result_name.returncode == 0 else ""
    user_email = (result_email.stdout or "").strip() if result_email.returncode == 0 else ""

    if not user_name:
        default_name = os.getenv("GIT_USER_NAME", "cnblogs-sync-bot")
        run_git(["config", "user.name", default_name], cwd=cwd, check=True)
    if not user_email:
        default_email = os.getenv("GIT_USER_EMAIL", "cnblogs-sync-bot@users.noreply.github.com")
        run_git(["config", "user.email", default_email], cwd=cwd, check=True)

def restore_state_from_git() -> str:
    """ä»ä¸“ç”¨åˆ†æ”¯æ¢å¤çŠ¶æ€æ–‡ä»¶ï¼ˆè®°å½• + å¢é‡çŠ¶æ€ï¼‰"""
    if not SYNC_STATE_GIT:
        return "æœªå¯ç”¨"
    if not is_git_repo():
        print("âš ï¸ å½“å‰ç›®å½•ä¸æ˜¯ Git ä»“åº“ï¼Œæ— æ³•ä»åˆ†æ”¯æ¢å¤çŠ¶æ€")
        return "è·³è¿‡ï¼ˆé Git ä»“åº“ï¼‰"

    try:
        ensure_remote(SYNC_STATE_REMOTE, SYNC_STATE_REMOTE_URL)
        if not has_remote(SYNC_STATE_REMOTE):
            print(f"âš ï¸ æœªæ‰¾åˆ° remote '{SYNC_STATE_REMOTE}'ï¼Œè¯·è®¾ç½® SYNC_REPO_TOKEN")
            return "è·³è¿‡ï¼ˆremote ç¼ºå¤±ï¼‰"
        run_git(["fetch", SYNC_STATE_REMOTE, SYNC_STATE_BRANCH], check=True)
    except Exception as e:
        print(f"âš ï¸ æ‹‰å–åˆ†æ”¯å¤±è´¥ï¼Œè·³è¿‡çŠ¶æ€æ¢å¤ï¼š{e}")
        return "è·³è¿‡ï¼ˆæ‹‰å–å¤±è´¥ï¼‰"

    restored = 0
    missing = 0
    skipped = 0
    for path in [SYNC_RECORD_FILE, SYNC_STATE_FILE, SYNC_RUN_LOG_FILE]:
        try:
            rel_path = path.relative_to(REPO_ROOT).as_posix()
        except ValueError:
            print(f"âš ï¸ çŠ¶æ€æ–‡ä»¶ä¸åœ¨ä»“åº“å†…ï¼Œè·³è¿‡æ¢å¤: {path}")
            skipped += 1
            continue
        result = run_git(["show", f"{SYNC_STATE_REMOTE}/{SYNC_STATE_BRANCH}:{rel_path}"])
        if result.returncode == 0:
            path.parent.mkdir(parents=True, exist_ok=True)
            path.write_text(result.stdout, encoding="utf-8")
            print(f"âœ… å·²ä»åˆ†æ”¯æ¢å¤çŠ¶æ€æ–‡ä»¶: {rel_path}")
            restored += 1
        else:
            print(f"â„¹ï¸ åˆ†æ”¯æœªåŒ…å«çŠ¶æ€æ–‡ä»¶: {rel_path}")
            missing += 1

    if restored == 0 and missing == 0 and skipped == 0:
        return "æ— çŠ¶æ€æ–‡ä»¶"

    parts = []
    if restored:
        parts.append(f"æ¢å¤={restored}")
    if missing:
        parts.append(f"ç¼ºå¤±={missing}")
    if skipped:
        parts.append(f"è·³è¿‡={skipped}")
    return "ï¼Œ".join(parts)

def persist_state_to_git():
    """å°†çŠ¶æ€æ–‡ä»¶æäº¤åˆ°ä¸“ç”¨åˆ†æ”¯å¹¶æ¨é€"""
    if not SYNC_STATE_GIT:
        return True, "æœªå¯ç”¨"
    if not is_git_repo():
        print("âš ï¸ å½“å‰ç›®å½•ä¸æ˜¯ Git ä»“åº“ï¼Œæ— æ³•æŒä¹…åŒ–çŠ¶æ€")
        return False, "é Git ä»“åº“"

    temp_dir = None
    try:
        ensure_remote(SYNC_STATE_REMOTE, SYNC_STATE_REMOTE_URL)
        if not has_remote(SYNC_STATE_REMOTE):
            print(f"âš ï¸ æœªæ‰¾åˆ° remote '{SYNC_STATE_REMOTE}'ï¼Œè¯·è®¾ç½® SYNC_REPO_TOKEN")
            return False, f"ç¼ºå°‘ remote: {SYNC_STATE_REMOTE}"
        branch_exists = remote_branch_exists(SYNC_STATE_REMOTE, SYNC_STATE_BRANCH)
        base_ref = "HEAD"
        if branch_exists:
            try:
                run_git(["fetch", SYNC_STATE_REMOTE, SYNC_STATE_BRANCH], check=True)
                fetch_head = run_git(["rev-parse", "FETCH_HEAD"])
                if fetch_head.returncode == 0 and fetch_head.stdout.strip():
                    base_ref = fetch_head.stdout.strip()
                else:
                    remote_ref = f"{SYNC_STATE_REMOTE}/{SYNC_STATE_BRANCH}"
                    ref_check = run_git(["rev-parse", "--verify", remote_ref])
                    if ref_check.returncode == 0:
                        base_ref = remote_ref
                    else:
                        print(f"âš ï¸ æœªæ‰¾åˆ°è¿œç«¯å¼•ç”¨ {remote_ref}ï¼Œå°†æ”¹ä¸ºåˆ›å»ºæ–°åˆ†æ”¯")
            except Exception as e:
                print(f"âš ï¸ æ‹‰å–çŠ¶æ€åˆ†æ”¯å¤±è´¥ï¼Œå°†æ”¹ä¸ºåˆ›å»ºæ–°åˆ†æ”¯ï¼š{e}")
                base_ref = "HEAD"

        temp_dir = tempfile.mkdtemp(prefix="cnblogs-sync-state-")
        run_git(["worktree", "add", "-B", SYNC_STATE_BRANCH, temp_dir, base_ref], check=True)

        rel_paths = []
        for path in [SYNC_RECORD_FILE, SYNC_STATE_FILE, SYNC_RUN_LOG_FILE]:
            if not path.exists():
                continue
            try:
                rel_path = path.relative_to(REPO_ROOT)
            except ValueError:
                print(f"âš ï¸ çŠ¶æ€æ–‡ä»¶ä¸åœ¨ä»“åº“å†…ï¼Œæ— æ³•æŒä¹…åŒ–: {path}")
                continue
            dest_path = Path(temp_dir) / rel_path
            dest_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(path, dest_path)
            rel_paths.append(rel_path.as_posix())

        if not rel_paths:
            print("â„¹ï¸ æœªæ‰¾åˆ°å¯æŒä¹…åŒ–çš„çŠ¶æ€æ–‡ä»¶")
            return True, "æ— çŠ¶æ€æ–‡ä»¶"

        status = run_git(["status", "--porcelain"], cwd=temp_dir)
        if not status.stdout.strip():
            print("â„¹ï¸ çŠ¶æ€æ–‡ä»¶æ— å˜åŒ–ï¼Œæ— éœ€æäº¤")
            return True, "æ— å˜åŒ–"

        run_git(["add"] + rel_paths, cwd=temp_dir, check=True)
        ensure_git_identity(temp_dir)
        commit_msg = f"chore: update cnblogs sync state ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})"
        run_git(["commit", "-m", commit_msg], cwd=temp_dir, check=True)
        try:
            run_git(["push", SYNC_STATE_REMOTE, SYNC_STATE_BRANCH], cwd=temp_dir, check=True)
        except Exception as e:
            msg = str(e)
            if "non-fast-forward" in msg or "fetch first" in msg or "rejected" in msg:
                print("âš ï¸ æ¨é€è¢«æ‹’ç»ï¼ˆnon-fast-forwardï¼‰ï¼Œæ”¹ç”¨ --force-with-lease é‡è¯•")
                run_git(["push", "--force-with-lease", SYNC_STATE_REMOTE, SYNC_STATE_BRANCH], cwd=temp_dir, check=True)
            else:
                raise
        print(f"âœ… çŠ¶æ€å·²æ¨é€åˆ°åˆ†æ”¯: {SYNC_STATE_BRANCH}")
        return True, f"å·²æ¨é€åˆ°åˆ†æ”¯: {SYNC_STATE_BRANCH}"
    except Exception as e:
        print(f"âŒ æŒä¹…åŒ–çŠ¶æ€å¤±è´¥: {e}")
        return False, "æŒä¹…åŒ–å¤±è´¥"
    finally:
        if temp_dir:
            try:
                run_git(["worktree", "remove", temp_dir, "--force"])
            except Exception:
                pass
            try:
                if Path(temp_dir).exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
            except Exception:
                pass

def load_sync_state():
    """åŠ è½½å¢é‡åŒæ­¥çŠ¶æ€"""
    if SYNC_STATE_FILE.exists():
        try:
            with open(SYNC_STATE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½çŠ¶æ€æ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œå°†ä½¿ç”¨ç©ºçŠ¶æ€")
            return {}
    return {}

def save_sync_state(state):
    """ä¿å­˜å¢é‡åŒæ­¥çŠ¶æ€"""
    try:
        SYNC_STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(SYNC_STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜çŠ¶æ€æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def append_run_log(entry):
    """è¿½åŠ ä¸€è¡Œè¿è¡Œè®°å½•ï¼ˆJSONLï¼‰"""
    try:
        SYNC_RUN_LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(SYNC_RUN_LOG_FILE, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    except Exception as e:
        print(f"âš ï¸ å†™å…¥è¿è¡Œè®°å½•å¤±è´¥: {e}")

def get_head_commit():
    """è·å–å½“å‰ HEAD commit"""
    result = run_git(["rev-parse", "HEAD"])
    if result.returncode == 0:
        return result.stdout.strip()
    return None

def short_commit(commit: str | None) -> str:
    if not commit:
        return "æ— "
    return commit[:8]

def get_changed_markdown_files(last_commit, head_commit):
    """è·å–ä¸¤æ¬¡æäº¤ä¹‹é—´å˜æ›´çš„ Markdown æ–‡ä»¶"""
    if not last_commit or not head_commit:
        return None
    result = run_git(["diff", "--name-only", f"{last_commit}..{head_commit}", "--", "*.md"])
    if result.returncode != 0:
        print(f"âš ï¸ è·å–å¢é‡æ–‡ä»¶å¤±è´¥: {result.stderr.strip()}")
        return None

    changed_files = []
    for line in result.stdout.splitlines():
        rel = line.strip()
        if not rel:
            continue
        rel_path = Path(rel)
        if any(part in EXCLUDE_DIRS for part in rel_path.parts):
            continue
        abs_path = REPO_ROOT / rel_path
        if abs_path.exists():
            changed_files.append(str(abs_path))
    return changed_files

def find_all_markdown_files(root_dir=None):
    """é€’å½’æŸ¥æ‰¾ä»“åº“ä¸­æ‰€æœ‰çš„ Markdown æ–‡ä»¶"""
    if root_dir is None:
        # é»˜è®¤ä»è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰å¼€å§‹æ‰«æ
        root_dir = REPO_ROOT
    
    root_path = Path(root_dir).resolve()
    md_files = []
    
    print(f"ğŸ” å¼€å§‹æ‰«æ Markdown æ–‡ä»¶ï¼ˆä» {root_path} å¼€å§‹ï¼‰...")
    
    for file_path in root_path.rglob('*.md'):
        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„ä¸­æ˜¯å¦åŒ…å«éœ€è¦æ’é™¤çš„ç›®å½•
        relative_path = file_path.relative_to(root_path)
        path_parts = relative_path.parts
        
        # å¦‚æœè·¯å¾„çš„ä»»ä½•éƒ¨åˆ†åœ¨æ’é™¤åˆ—è¡¨ä¸­ï¼Œè·³è¿‡
        if any(part in EXCLUDE_DIRS for part in path_parts):
            continue
        
        md_files.append(str(file_path))
    
    md_files.sort()  # æŒ‰è·¯å¾„æ’åº
    print(f"âœ… æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")
    return md_files

def get_file_content(filepath):
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return f.read()

def replace_internal_md_links(content):
    """æŸ¥æ‰¾å†…å®¹ä¸­æ‰€æœ‰æŒ‡å‘æœ¬åœ° .md æ–‡ä»¶çš„é“¾æ¥ï¼Œå¹¶å°†å…¶æ›¿æ¢ä¸ºåšå®¢å›­ç«™å†…æœç´¢é“¾æ¥ã€‚"""
    md_link_pattern = re.compile(r'(\[.*?\])\((.*?\.md)\)')
    def replacer(match):
        link_text = match.group(1)
        md_path = match.group(2)
        keyword = os.path.basename(md_path).replace('.md', '')

        # æ ¸å¿ƒä¿®æ”¹ï¼šä¸å†å¯¹å…³é”®è¯è¿›è¡Œ URL ç¼–ç 
        # encoded_keyword = quote(keyword) # ç§»é™¤æ­¤è¡Œ

        # ç›´æ¥ä½¿ç”¨åŸå§‹å…³é”®è¯æ„å»º URL
        new_url = f"{CNBLOGS_SEARCH_URL}?Keywords={keyword}"
        return f"{link_text}({new_url} )"
    return md_link_pattern.sub(replacer, content)

def load_sync_record():
    """åŠ è½½æœ¬åœ°å‘å¸ƒè®°å½•"""
    if SYNC_RECORD_FILE.exists():
        try:
            with open(SYNC_RECORD_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å‘å¸ƒè®°å½•æ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œå°†ä½¿ç”¨ç©ºè®°å½•")
            return {}
    return {}

def save_sync_record(record):
    """ä¿å­˜æœ¬åœ°å‘å¸ƒè®°å½•"""
    try:
        SYNC_RECORD_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(SYNC_RECORD_FILE, 'w', encoding='utf-8') as f:
            json.dump(record, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å‘å¸ƒè®°å½•æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def get_blog_id(server):
    """è‡ªåŠ¨è·å– BLOG_ID"""
    try:
        blogs = server.blogger.getUsersBlogs('', USERNAME, PASSWORD)
        if blogs and len(blogs) > 0:
            blog = blogs[0] or {}
            blog_id = blog.get('blogid') or blog.get('blogId') or blog.get('id')
            return str(blog_id) if blog_id is not None else None
    except Exception as e:
        print(f"âš ï¸ è‡ªåŠ¨è·å– BLOG_ID å¤±è´¥: {e}")
    return None

def init_sync_record():
    """åˆå§‹åŒ–å‘å¸ƒè®°å½•ï¼šä» API è·å–æœ€è¿‘ 300 ç¯‡æ–‡ç« çš„æ ‡é¢˜å’Œ post_id"""
    global BLOG_ID
    if not all([RPC_URL, USERNAME, PASSWORD]):
        print("âŒ é”™è¯¯ï¼šä¸€ä¸ªæˆ–å¤šä¸ªç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæ— æ³•åˆå§‹åŒ–å‘å¸ƒè®°å½•")
        return False
    
    print("ğŸ”„ å¼€å§‹åˆå§‹åŒ–å‘å¸ƒè®°å½•ï¼ˆä» API è·å–æœ€è¿‘ 300 ç¯‡æ–‡ç« ï¼‰...")
    
    try:
        server = xmlrpc.client.ServerProxy(RPC_URL)
        if not BLOG_ID:
            BLOG_ID = get_blog_id(server)
            if not BLOG_ID:
                print("âŒ é”™è¯¯ï¼šæ— æ³•è‡ªåŠ¨è·å– BLOG_IDï¼Œè¯·æ£€æŸ¥è´¦å·æƒé™ä¸ RPC é…ç½®")
                return False
            print(f"âœ… è‡ªåŠ¨è·å–åˆ° BLOG_ID: {BLOG_ID}")

        # API æé™æ˜¯ 300 ç¯‡
        recent_posts = server.metaWeblog.getRecentPosts(BLOG_ID, USERNAME, PASSWORD, 300)
        
        if not recent_posts:
            record = load_sync_record() or {}
            save_sync_record(record)
            print("â„¹ï¸ æœªè·å–åˆ°ä»»ä½•æ–‡ç« ï¼Œå·²åˆå§‹åŒ–ç©ºå‘å¸ƒè®°å½•")
            print(f"ğŸ“ è®°å½•æ–‡ä»¶ä¿å­˜åœ¨: {SYNC_RECORD_FILE}")
            return True
        
        # åˆå¹¶æ¨¡å¼ï¼šä¿ç•™æ—§è®°å½• + ç”¨æœ€è¿‘ 300 ç¯‡åˆ·æ–° post_idï¼ˆé¿å…å›  API é™åˆ¶ä¸¢å¤±æ—§æ˜ å°„ï¼‰
        record = load_sync_record() or {}
        for post in recent_posts:
            title = post.get('title', '').strip()
            post_id = post.get('postid')
            if title and post_id:
                record[title] = post_id
        
        # ä¿å­˜è®°å½•
        save_sync_record(record)
        print(f"âœ… æˆåŠŸåˆå§‹åŒ–å‘å¸ƒè®°å½•ï¼šå…± {len(record)} ç¯‡æ–‡ç« ")
        print(f"ğŸ“ è®°å½•æ–‡ä»¶ä¿å­˜åœ¨: {SYNC_RECORD_FILE}")
        return True
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å‘å¸ƒè®°å½•æ—¶å‡ºé”™: {e}")
        return False

PostResult = Literal["created", "updated", "skipped", "failed"]


def post_to_cnblogs(title, content, categories=None) -> PostResult:
    """å‘å¸ƒæ–‡ç« åˆ°åšå®¢å›­ï¼ŒåŸºäºæœ¬åœ°è®°å½•åˆ¤æ–­æ˜¯å¦å·²å­˜åœ¨"""
    # --- æ­¥éª¤1: å‡†å¤‡æœ€ç»ˆå†…å®¹ ---

    # æ ¸å¿ƒä¿®æ”¹ï¼šä¸å†å¯¹æ ‡é¢˜è¿›è¡Œ URL ç¼–ç 
    # encoded_title = quote(title) # ç§»é™¤æ­¤è¡Œ

    # ç›´æ¥ä½¿ç”¨åŸå§‹æ ‡é¢˜æ„å»º URL
    knowledge_base_url = f"{KNOWLEDGE_BASE_URL}?q={title}"
    prepend_content = f"> å…³è”çŸ¥è¯†åº“ï¼š<a href=\"{knowledge_base_url}\">{title}</a>\r\n\r\n"

    processed_body = replace_internal_md_links(content)
    final_content = prepend_content + processed_body

    # --- æ­¥éª¤2: å‡†å¤‡ post æ•°æ®ç»“æ„ ---
    final_categories = ['[Markdown]']
    if categories and isinstance(categories, list):
        final_categories.extend(categories)
    else:
        final_categories.append('[éšç¬”åˆ†ç±»]')

    post_data = {
        'title': title,
        'description': final_content,
        'categories': final_categories,
        'publish': True
    }

    # --- æ­¥éª¤3: åŸºäºæœ¬åœ°è®°å½•çš„æ ¸å¿ƒå‘å¸ƒ/æ›´æ–°/è·³è¿‡é€»è¾‘ ---
    try:
        server = xmlrpc.client.ServerProxy(RPC_URL)
        
        # åŠ è½½æœ¬åœ°è®°å½•
        sync_record = load_sync_record()
        existing_post_id = sync_record.get(title)

        if existing_post_id:
            if FORCE_OVERWRITE_EXISTING:
                print(f"â„¹ï¸ æœ¬åœ°è®°å½•æ˜¾ç¤ºæ–‡ç«  '{title}' å·²å­˜åœ¨ï¼ˆPost ID: {existing_post_id}ï¼‰ï¼Œå¼ºåˆ¶è¦†ç›–æ¨¡å¼å·²å¼€å¯...")
                success = server.metaWeblog.editPost(existing_post_id, USERNAME, PASSWORD, post_data, post_data['publish'])
                if success:
                    print(f"âœ… æˆåŠŸæ›´æ–°æ–‡ç«  '{title}'ï¼ŒPost ID: {existing_post_id}")
                    # ç¡®ä¿è®°å½•ä¸­çš„ post_id æ˜¯æœ€æ–°çš„ï¼ˆè™½ç„¶é€šå¸¸ä¸ä¼šå˜ï¼‰
                    sync_record[title] = existing_post_id
                    save_sync_record(sync_record)
                    return "updated"
                else:
                    print(f"âŒ æ›´æ–°æ–‡ç«  '{title}' å¤±è´¥")
                    return "failed"
            else:
                print(f"â„¹ï¸ æœ¬åœ°è®°å½•æ˜¾ç¤ºæ–‡ç«  '{title}' å·²å­˜åœ¨ï¼ˆPost ID: {existing_post_id}ï¼‰ï¼Œè·³è¿‡å‘å¸ƒ")
                return "skipped"
        else:
            print(f"ğŸ“„ æ–‡ç«  '{title}' ä¸åœ¨æœ¬åœ°è®°å½•ä¸­ï¼Œå°†åˆ›å»ºæ–°æ–‡ç« ")
            new_post_id = server.metaWeblog.newPost(BLOG_ID, USERNAME, PASSWORD, post_data, post_data['publish'])
            print(f"âœ… æˆåŠŸå‘å¸ƒæ–°æ–‡ç«  '{title}'ï¼Œæ–‡ç« ID: {new_post_id}")
            
            # æ›´æ–°æœ¬åœ°è®°å½•ï¼ˆå§‹ç»ˆæ›´æ–°ï¼‰
            sync_record[title] = new_post_id
            save_sync_record(sync_record)
            return "created"

    except Exception as e:
        print(f"âŒ å‘å¸ƒæˆ–æ›´æ–°æ–‡ç«  '{title}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return "failed"

# --- ä¸»æµç¨‹ ---
if __name__ == "__main__":
    run_started_ts = time.time()
    missing_vars = []
    if not RPC_URL:
        missing_vars.append("CNBLOGS_RPC_URL")
    if not USERNAME:
        missing_vars.append("CNBLOGS_USERNAME")
    if not PASSWORD:
        missing_vars.append("CNBLOGS_TOKEN")
    if not SYNC_REPO_URL:
        missing_vars.append("SYNC_REPO_URL")
    if not SYNC_REPO_TOKEN:
        missing_vars.append("SYNC_REPO_TOKEN")

    if missing_vars:
        print("âŒ ç¯å¢ƒå˜é‡ç¼ºå¤±ï¼Œæ— æ³•ç»§ç»­ï¼š")
        for var in missing_vars:
            print(f"  - {var}")
        print("è¯·æ£€æŸ¥ .env æˆ–ç³»ç»Ÿç¯å¢ƒå˜é‡åå†è¿è¡Œã€‚")
        sys.exit(1)

    log_plan()
    step_status = ["æœªå¼€å§‹"] * len(SYNC_STEPS)

    def set_status(step_index: int, status: str, detail: str | None = None) -> None:
        if detail:
            step_status[step_index - 1] = f"{status}ï¼š{detail}"
        else:
            step_status[step_index - 1] = status

    def print_summary() -> None:
        print("\næ‰§è¡Œç»“æœï¼š")
        for i, title in enumerate(SYNC_STEPS, 1):
            print(f"  {i}. {title} -> {step_status[i - 1]}")

    # Step 1: prepare & restore
    step = 1
    log_step_start(step)
    restore_detail = restore_state_from_git()
    if not BLOG_ID:
        try:
            server = xmlrpc.client.ServerProxy(RPC_URL)
            BLOG_ID = get_blog_id(server)
            if BLOG_ID:
                print(f"âœ… è‡ªåŠ¨è·å–åˆ° BLOG_ID: {BLOG_ID}")
            else:
                log_step_fail(step, "æ— æ³•è‡ªåŠ¨è·å– BLOG_ID")
                set_status(step, "å¤±è´¥", "BLOG_ID è·å–å¤±è´¥")
                print_summary()
                sys.exit(1)
        except Exception as e:
            log_step_fail(step, f"è·å– BLOG_ID å¤±è´¥: {e}")
            set_status(step, "å¤±è´¥", "BLOG_ID è·å–å¼‚å¸¸")
            print_summary()
            sys.exit(1)
    step1_detail = f"çŠ¶æ€æ¢å¤={restore_detail}ï¼ŒBLOG_ID={BLOG_ID}"
    log_step_ok(step, step1_detail)
    set_status(step, "æˆåŠŸ", step1_detail)

    # Step 2: init record
    step = 2
    log_step_start(step)
    if not SYNC_RECORD_FILE.exists():
        print("  - å‘å¸ƒè®°å½•ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆå§‹åŒ–")
        ok = init_sync_record()
        if not ok:
            log_step_fail(step, "åˆå§‹åŒ–å‘å¸ƒè®°å½•å¤±è´¥")
            set_status(step, "å¤±è´¥", "åˆå§‹åŒ–å¤±è´¥")
            print_summary()
            sys.exit(1)
        record_count = len(load_sync_record() or {})
        record_detail = f"è®°å½•æ•°={record_count}"
        log_step_ok(step, record_detail)
        set_status(step, "æˆåŠŸ", record_detail)
    else:
        log_step_skip(step, "å‘å¸ƒè®°å½•å·²å­˜åœ¨")
        set_status(step, "è·³è¿‡", "å‘å¸ƒè®°å½•å·²å­˜åœ¨")

    # Step 3: build publish list
    step = 3
    log_step_start(step)
    sync_state = load_sync_state()
    head_commit = get_head_commit()
    last_synced_commit = sync_state.get("last_synced_commit")
    head_short = short_commit(head_commit)
    last_short = short_commit(last_synced_commit)

    run_mode = "full"
    if len(sys.argv) > 1:
        files_to_publish = sys.argv[1:]
        print(f"  - æ‰‹åŠ¨æ¨¡å¼ï¼šæŒ‡å®š {len(files_to_publish)} ä¸ªæ–‡ä»¶")
        run_mode = "manual"
    else:
        files_to_publish = None
        if INCREMENTAL_SYNC:
            changed_files = get_changed_markdown_files(last_synced_commit, head_commit)
            if changed_files is not None:
                files_to_publish = changed_files
                run_mode = "incremental"
                print(f"  - å¢é‡å¯¹æ¯”ï¼š{last_short}..{head_short}ï¼Œå˜æ›´ {len(files_to_publish)} ä¸ª Markdown æ–‡ä»¶")
            else:
                print("  - å¢é‡å·®å¼‚è·å–å¤±è´¥ï¼Œæ”¹ä¸ºå…¨é‡æ‰«æ")

        if files_to_publish is None:
            files_to_publish = find_all_markdown_files()
            if not files_to_publish:
                log_step_ok(step, "æœªæ‰¾åˆ° Markdown æ–‡ä»¶")
                set_status(step, "è·³è¿‡", "æœªæ‰¾åˆ° Markdown æ–‡ä»¶")
                print_summary()
                sys.exit(0)
            print(f"  - å…¨é‡æ‰«æï¼šå…± {len(files_to_publish)} ä¸ª Markdown æ–‡ä»¶")

    list_detail = f"æ¨¡å¼={run_mode}ï¼Œå€™é€‰={len(files_to_publish)}"
    if run_mode != "manual":
        list_detail += f"ï¼Œlast={last_short}ï¼Œhead={head_short}"
    log_step_ok(step, list_detail)
    set_status(step, "æˆåŠŸ", list_detail)

    # Step 4: publish
    step = 4
    log_step_start(step)
    if run_mode == "incremental" and not files_to_publish:
        step4_detail = "æ— å˜æ›´"
        log_step_skip(step, step4_detail)
        set_status(step, "è·³è¿‡", step4_detail)

        if head_commit:
            sync_state["last_synced_commit"] = head_commit
        sync_state["last_run_at"] = datetime.now().isoformat(timespec="seconds")
        sync_state["last_run_mode"] = run_mode
        sync_state["last_total_candidates"] = 0
        sync_state["last_published_count"] = 0
        sync_state["last_skipped_count"] = 0
        sync_state["last_failed_count"] = 0
        save_sync_state(sync_state)
        log_entry = {
            "ts": datetime.now().isoformat(timespec="seconds"),
            "mode": run_mode,
            "candidates": 0,
            "published": 0,
            "skipped": 0,
            "failed": 0,
            "status": "no_change",
            "duration_s": int(time.time() - run_started_ts)
        }
        if head_commit:
            log_entry["head_commit"] = head_commit
        append_run_log(log_entry)

        step = 5
        log_step_start(step)
        persist_ok, persist_detail = persist_state_to_git()
        if not persist_ok:
            log_step_fail(step, persist_detail)
            set_status(step, "å¤±è´¥", persist_detail)
            print_summary()
            sys.exit(2)
        log_step_ok(step, persist_detail)
        set_status(step, "æˆåŠŸ", persist_detail)
        print_summary()
        sys.exit(0)

    SUCCESS_BATCH_SIZE_SMALL = 5
    SUCCESS_REST_SECONDS_SMALL = 3
    SUCCESS_BATCH_SIZE_LARGE = 20
    SUCCESS_REST_SECONDS_LARGE = 10

    success_count = 0
    skipped_count = 0
    failed_count = 0
    missing_count = 0

    for idx, md_file in enumerate(files_to_publish, 1):
        if not os.path.exists(md_file):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: '{md_file}'")
            failed_count += 1
            missing_count += 1
            continue

        print(f"\n[{idx}/{len(files_to_publish)}] å¤„ç†æ–‡ä»¶: {md_file}")
        post_title = os.path.basename(md_file).replace('.md', '')
        post_content = get_file_content(md_file)

        result = post_to_cnblogs(post_title, post_content)

        if result in {"created", "updated"}:
            success_count += 1
            if success_count % SUCCESS_BATCH_SIZE_SMALL == 0:
                print(f"\nâ³ å·²å¤„ç† {success_count} ç¯‡ï¼Œä¼‘æ¯ {SUCCESS_REST_SECONDS_SMALL}s...")
                time.sleep(SUCCESS_REST_SECONDS_SMALL)
                print("âœ… ç»§ç»­åŒæ­¥...\n")

            if success_count % SUCCESS_BATCH_SIZE_LARGE == 0:
                print(f"\nâ³ å·²å¤„ç† {success_count} ç¯‡ï¼Œä¼‘æ¯ {SUCCESS_REST_SECONDS_LARGE}s...")
                time.sleep(SUCCESS_REST_SECONDS_LARGE)
                print("âœ… ç»§ç»­åŒæ­¥...\n")
        elif result == "skipped":
            skipped_count += 1
        else:
            failed_count += 1

    step4_detail = (
        f"æˆåŠŸ={success_count}ï¼Œè·³è¿‡={skipped_count}ï¼Œå¤±è´¥={failed_count}ï¼Œæ€»è®¡={len(files_to_publish)}"
    )
    if missing_count:
        step4_detail += f"ï¼Œç¼ºå¤±={missing_count}"
    log_step_ok(step, step4_detail)
    step4_status = "æˆåŠŸ" if failed_count == 0 else "éƒ¨åˆ†å¤±è´¥"
    set_status(step, step4_status, step4_detail)

    if run_mode != "manual" and head_commit:
        sync_state["last_synced_commit"] = head_commit
    sync_state["last_run_at"] = datetime.now().isoformat(timespec="seconds")
    sync_state["last_run_mode"] = run_mode
    sync_state["last_total_candidates"] = len(files_to_publish)
    sync_state["last_published_count"] = success_count
    sync_state["last_skipped_count"] = skipped_count
    sync_state["last_failed_count"] = failed_count
    save_sync_state(sync_state)
    log_entry = {
        "ts": datetime.now().isoformat(timespec="seconds"),
        "mode": run_mode,
        "candidates": len(files_to_publish),
        "published": success_count,
        "skipped": skipped_count,
        "failed": failed_count,
        "status": "completed",
        "duration_s": int(time.time() - run_started_ts)
    }
    if head_commit:
        log_entry["head_commit"] = head_commit
    append_run_log(log_entry)

    step = 5
    log_step_start(step)
    persist_ok, persist_detail = persist_state_to_git()
    if not persist_ok:
        log_step_fail(step, persist_detail)
        set_status(step, "å¤±è´¥", persist_detail)
        print_summary()
        sys.exit(2)
    log_step_ok(step, persist_detail)
    set_status(step, "æˆåŠŸ", persist_detail)

    print_summary()
