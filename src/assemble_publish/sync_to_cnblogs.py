# sync_to_cnblogs.py
#
# åšå®¢å›­æ–‡ç« å‘å¸ƒè„šæœ¬
#
# ã€åŠŸèƒ½è¯´æ˜ã€‘
# å°†æœ¬åœ° Markdown æ–‡ä»¶å‘å¸ƒåˆ°åšå®¢å›­ï¼ˆå•å‘ï¼šæœ¬åœ° â†’ åšå®¢å›­ï¼‰ã€‚
#
# ã€ç¯å¢ƒå˜é‡é…ç½®ã€‘
# ä½¿ç”¨å‰éœ€è¦è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼ˆé€šè¿‡ .env æ–‡ä»¶æˆ–ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼‰ï¼š
#   - CNBLOGS_RPC_URL: åšå®¢å›­ RPC åœ°å€ï¼ˆå¿…éœ€ï¼‰
#   - CNBLOGS_USERNAME: ç”¨æˆ·åï¼ˆå¿…éœ€ï¼‰
#   - CNBLOGS_TOKEN: Tokenï¼ˆå¿…éœ€ï¼‰
#
# ã€æ— çŠ¶æ€è¯´æ˜ã€‘
# - ä¸å†™å…¥ä»»ä½•æœ¬åœ°è®°å½•/çŠ¶æ€æ–‡ä»¶
# - æ¯æ¬¡è¿è¡Œä»…åŸºäº API æœ€è¿‘ 300 ç¯‡åˆ¤æ–­æ˜¯å¦æ›´æ–°æˆ–æ–°å»º

import os
import sys
import re
import time
import xmlrpc.client
from pathlib import Path
from typing import Literal
from dotenv import load_dotenv

# æ”¯æŒç›´æ¥æ‰§è¡Œå’Œä½œä¸ºæ¨¡å—å¯¼å…¥
try:
    from .common import logger
except ImportError:
    # ç›´æ¥æ‰§è¡Œæ—¶ï¼Œæ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
    sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
    from assemble_publish.common import logger


class DailyLimitReached(Exception):
    """åšå®¢å›­å½“æ—¥å‘å¸ƒæ•°é‡è¾¾åˆ°ä¸Šé™"""

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# --- é…ç½®ä¿¡æ¯ï¼ˆä»…ä¿ç•™å¿…éœ€é¡¹ï¼‰ ---
RPC_URL = os.getenv("CNBLOGS_RPC_URL")
USERNAME = os.getenv("CNBLOGS_USERNAME")
PASSWORD = os.getenv("CNBLOGS_TOKEN")


# ä¸‹é¢ä¸ºå›ºå®šé»˜è®¤å€¼ï¼Œä¸å¯¹å¤–æš´éœ²é…ç½®
BLOG_ID = None  # è‡ªåŠ¨è·å–
KNOWLEDGE_BASE_URL = "https://assemble.gitbook.io/assemble"
CNBLOGS_SEARCH_URL = "https://zzk.cnblogs.com/my/s/blogpost-p"
RECENT_POSTS_MAP: dict[str, str] = {}

# --- Git / è¿è¡Œç¯å¢ƒå°ä¼˜åŒ– ---
# é¿å…åœ¨æ— äº¤äº’ç¯å¢ƒï¼ˆZeabur/Cronï¼‰é‡Œ git push è§¦å‘å‡­æ®äº¤äº’å¡æ­»
os.environ.setdefault("GIT_TERMINAL_PROMPT", "0")

# --- è¡Œä¸ºå¼€å…³ ---
FORCE_OVERWRITE_EXISTING = True

# --- ä»“åº“æ ¹ç›®å½•ï¼ˆæ”¯æŒå¤–éƒ¨ä¼ å…¥ï¼‰ ---
REPO_ROOT = Path.cwd().resolve()

SYNC_STEPS = [
    "å‡†å¤‡",
    "è·å–æœ€è¿‘æ–‡ç« æ˜ å°„",
    "ç”Ÿæˆå¾…å‘å¸ƒåˆ—è¡¨",
    "å‘å¸ƒ/æ›´æ–°æ–‡ç« ",
]


def log_plan():
    logger.info("æ‰§è¡Œè®¡åˆ’ï¼ˆåŒæ­¥æµç¨‹ï¼‰ï¼š")
    for i, title in enumerate(SYNC_STEPS, 1):
        logger.info(f"  {i}. {title}")


def log_step_start(step_index: int) -> None:
    logger.info(f"[{step_index}/{len(SYNC_STEPS)}] {SYNC_STEPS[step_index - 1]}")


def log_step_ok(step_index: int, detail: str | None = None) -> None:
    title = SYNC_STEPS[step_index - 1]
    if detail:
        logger.info(f"âœ… {title}ï¼š{detail}")
    else:
        logger.info(f"âœ… {title} å®Œæˆ")


def log_step_skip(step_index: int, detail: str | None = None) -> None:
    title = SYNC_STEPS[step_index - 1]
    if detail:
        logger.info(f"â­ï¸ {title}ï¼š{detail}")
    else:
        logger.info(f"â­ï¸ {title} è·³è¿‡")


def log_step_fail(step_index: int, detail: str) -> None:
    title = SYNC_STEPS[step_index - 1]
    logger.error(f"âŒ {title} å¤±è´¥ï¼š{detail}")

# --- éœ€è¦æ’é™¤çš„ç›®å½•ï¼ˆä¸æ‰«æè¿™äº›ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼‰ ---
EXCLUDE_DIRS = {'.git', '.github', 'node_modules', '__pycache__', '.vscode', '.idea', 'cnblogs_sync', '.cnblogs_sync'}

# --- å‡½æ•°å®šä¹‰ ---

def find_all_markdown_files(root_dir=None):
    """é€’å½’æŸ¥æ‰¾ä»“åº“ä¸­æ‰€æœ‰çš„ Markdown æ–‡ä»¶"""
    if root_dir is None:
        root_dir = REPO_ROOT

    root_path = Path(root_dir).resolve()
    md_files = []

    logger.info(f"ğŸ” å¼€å§‹æ‰«æ Markdown æ–‡ä»¶ï¼ˆä» {root_path} å¼€å§‹ï¼‰...")

    for file_path in root_path.rglob('*.md'):
        relative_path = file_path.relative_to(root_path)
        path_parts = relative_path.parts

        if any(part in EXCLUDE_DIRS for part in path_parts):
            continue

        md_files.append(str(file_path))

    md_files.sort()
    logger.info(f"âœ… æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")
    return md_files

def get_file_content(filepath):
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return f.read()

def replace_internal_md_links(content):
    """æŸ¥æ‰¾å†…å®¹ä¸­æ‰€æœ‰æŒ‡å‘æœ¬åœ° .md æ–‡ä»¶çš„é“¾æ¥ï¼Œå¹¶å°†å…¶æ›¿æ¢ä¸ºåšå®¢å›­ç«™å†…æœç´¢é“¾æ¥ã€‚"""
    md_link_pattern = re.compile(r'(\[.*?\])\((.*?\.md)\)')
    def replacer(match):
        link_text = match.group(1)
        md_path = match.group(2)
        keyword = os.path.basename(md_path).replace('.md', '')

        # æ ¸å¿ƒä¿®æ”¹ï¼šä¸å†å¯¹å…³é”®è¯è¿›è¡Œ URL ç¼–ç 
        # encoded_keyword = quote(keyword) # ç§»é™¤æ­¤è¡Œ

        # ç›´æ¥ä½¿ç”¨åŸå§‹å…³é”®è¯æ„å»º URL
        new_url = f"{CNBLOGS_SEARCH_URL}?Keywords={keyword}"
        return f"{link_text}({new_url} )"
    return md_link_pattern.sub(replacer, content)

def get_blog_id(server):
    """è‡ªåŠ¨è·å– BLOG_ID"""
    try:
        blogs = server.blogger.getUsersBlogs('', USERNAME, PASSWORD)
        if blogs and len(blogs) > 0:
            blog = blogs[0] or {}
            blog_id = blog.get('blogid') or blog.get('blogId') or blog.get('id')
            return str(blog_id) if blog_id is not None else None
    except Exception as e:
        logger.warning(f"è‡ªåŠ¨è·å– BLOG_ID å¤±è´¥: {e}")
    return None

def fetch_recent_posts_map(server, limit=300):
    """è·å–æœ€è¿‘æ–‡ç« æ˜ å°„ï¼ˆæ ‡é¢˜ -> post_idï¼‰ï¼Œä»…ç”¨äºæœ¬æ¬¡è¿è¡Œã€‚å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸ã€‚"""
    recent_posts = server.metaWeblog.getRecentPosts(BLOG_ID, USERNAME, PASSWORD, limit)

    mapping = {}
    for post in (recent_posts or []):
        title = post.get('title', '').strip()
        post_id = post.get('postid')
        if title and post_id:
            mapping[title] = post_id
    return mapping

PostResult = Literal["created", "updated", "skipped", "failed"]


def post_to_cnblogs(title, content, categories=None) -> PostResult:
    """å‘å¸ƒæ–‡ç« åˆ°åšå®¢å›­ï¼ŒåŸºäºæœ€è¿‘æ–‡ç« æ˜ å°„åˆ¤æ–­æ˜¯å¦å·²å­˜åœ¨"""
    knowledge_base_url = f"{KNOWLEDGE_BASE_URL}?q={title}"
    prepend_content = f"> å…³è”çŸ¥è¯†åº“ï¼š<a href=\"{knowledge_base_url}\">{title}</a>\r\n\r\n"

    processed_body = replace_internal_md_links(content)
    final_content = prepend_content + processed_body

    final_categories = ['[Markdown]']
    if categories and isinstance(categories, list):
        final_categories.extend(categories)
    else:
        final_categories.append('[éšç¬”åˆ†ç±»]')

    post_data = {
        'title': title,
        'description': final_content,
        'categories': final_categories,
        'publish': True
    }

    try:
        server = xmlrpc.client.ServerProxy(RPC_URL)
        existing_post_id = RECENT_POSTS_MAP.get(title)

        if existing_post_id:
            if FORCE_OVERWRITE_EXISTING:
                logger.info(f"â„¹ï¸ æœ€è¿‘æ–‡ç« ä¸­å·²å­˜åœ¨ '{title}'ï¼ˆPost ID: {existing_post_id}ï¼‰ï¼Œå¼ºåˆ¶è¦†ç›–...")
                success = server.metaWeblog.editPost(existing_post_id, USERNAME, PASSWORD, post_data, post_data['publish'])
                if success:
                    logger.info(f"âœ… æˆåŠŸæ›´æ–°æ–‡ç«  '{title}'ï¼ŒPost ID: {existing_post_id}")
                    RECENT_POSTS_MAP[title] = existing_post_id
                    return "updated"
                else:
                    logger.error(f"âŒ æ›´æ–°æ–‡ç«  '{title}' å¤±è´¥")
                    return "failed"
            else:
                logger.info(f"â„¹ï¸ æœ€è¿‘æ–‡ç« ä¸­å·²å­˜åœ¨ '{title}'ï¼ˆPost ID: {existing_post_id}ï¼‰ï¼Œè·³è¿‡å‘å¸ƒ")
                return "skipped"
        else:
            logger.info(f"ğŸ“„ æ–‡ç«  '{title}' ä¸åœ¨æœ€è¿‘æ–‡ç« ä¸­ï¼Œå°†åˆ›å»ºæ–°æ–‡ç« ")
            new_post_id = server.metaWeblog.newPost(BLOG_ID, USERNAME, PASSWORD, post_data, post_data['publish'])
            logger.info(f"âœ… æˆåŠŸå‘å¸ƒæ–°æ–‡ç«  '{title}'ï¼Œæ–‡ç« ID: {new_post_id}")
            RECENT_POSTS_MAP[title] = new_post_id
            return "created"

    except xmlrpc.client.Fault as e:
        msg = str(e)
        if "å½“æ—¥åšæ–‡å‘å¸ƒæ•°é‡" in msg or "è¶…å‡ºå½“æ—¥åšæ–‡å‘å¸ƒæ•°é‡" in msg:
            raise DailyLimitReached(msg)
        logger.error(f"âŒ å‘å¸ƒæˆ–æ›´æ–°æ–‡ç«  '{title}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return "failed"
    except Exception as e:
        logger.error(f"âŒ å‘å¸ƒæˆ–æ›´æ–°æ–‡ç«  '{title}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return "failed"

# --- ä¸»æµç¨‹ ---
if __name__ == "__main__":
    run_started_ts = time.time()
    missing_vars = []
    if not RPC_URL:
        missing_vars.append("CNBLOGS_RPC_URL")
    if not USERNAME:
        missing_vars.append("CNBLOGS_USERNAME")
    if not PASSWORD:
        missing_vars.append("CNBLOGS_TOKEN")

    if missing_vars:
        logger.error("âŒ ç¯å¢ƒå˜é‡ç¼ºå¤±ï¼Œæ— æ³•ç»§ç»­ï¼š")
        for var in missing_vars:
            logger.error(f"  - {var}")
        logger.error("è¯·æ£€æŸ¥ .env æˆ–ç³»ç»Ÿç¯å¢ƒå˜é‡åå†è¿è¡Œã€‚")
        sys.exit(1)

    log_plan()
    step_status = ["æœªå¼€å§‹"] * len(SYNC_STEPS)

    def set_status(step_index: int, status: str, detail: str | None = None) -> None:
        if detail:
            step_status[step_index - 1] = f"{status}ï¼š{detail}"
        else:
            step_status[step_index - 1] = status

    def print_summary() -> None:
        logger.info("æ‰§è¡Œç»“æœï¼š")
        for i, title in enumerate(SYNC_STEPS, 1):
            logger.info(f"  {i}. {title} -> {step_status[i - 1]}")

    # Step 1: prepare
    step = 1
    log_step_start(step)
    server = xmlrpc.client.ServerProxy(RPC_URL)
    if not BLOG_ID:
        try:
            BLOG_ID = get_blog_id(server)
            if BLOG_ID:
                logger.info(f"âœ… è‡ªåŠ¨è·å–åˆ° BLOG_ID: {BLOG_ID}")
            else:
                log_step_fail(step, "æ— æ³•è‡ªåŠ¨è·å– BLOG_ID")
                set_status(step, "å¤±è´¥", "BLOG_ID è·å–å¤±è´¥")
                print_summary()
                sys.exit(1)
        except Exception as e:
            log_step_fail(step, f"è·å– BLOG_ID å¤±è´¥: {e}")
            set_status(step, "å¤±è´¥", "BLOG_ID è·å–å¼‚å¸¸")
            print_summary()
            sys.exit(1)
    step1_detail = f"BLOG_ID={BLOG_ID}"
    log_step_ok(step, step1_detail)
    set_status(step, "æˆåŠŸ", step1_detail)

    # Step 2: fetch recent posts map
    step = 2
    log_step_start(step)
    RECENT_POSTS_MAP.clear()
    try:
        RECENT_POSTS_MAP.update(fetch_recent_posts_map(server, limit=300))
    except Exception as e:
        log_step_fail(step, f"è·å–æœ€è¿‘æ–‡ç« å¤±è´¥: {e}")
        set_status(step, "å¤±è´¥", "API è°ƒç”¨å¤±è´¥")
        print_summary()
        sys.exit(1)
    record_count = len(RECENT_POSTS_MAP)
    record_detail = f"å·²è·å–æœ€è¿‘ {record_count} ç¯‡æ–‡ç« "
    log_step_ok(step, record_detail)
    set_status(step, "æˆåŠŸ", record_detail)

    # Step 3: build publish list
    step = 3
    log_step_start(step)
    run_mode = "full"
    if len(sys.argv) > 1:
        files_to_publish = sys.argv[1:]
        logger.info(f"  - æ‰‹åŠ¨æ¨¡å¼ï¼šæŒ‡å®š {len(files_to_publish)} ä¸ªæ–‡ä»¶")
        run_mode = "manual"
    else:
        files_to_publish = find_all_markdown_files()
        if not files_to_publish:
            log_step_ok(step, "æœªæ‰¾åˆ° Markdown æ–‡ä»¶")
            set_status(step, "è·³è¿‡", "æœªæ‰¾åˆ° Markdown æ–‡ä»¶")
            print_summary()
            sys.exit(0)
        logger.info(f"  - å…¨é‡æ‰«æï¼šå…± {len(files_to_publish)} ä¸ª Markdown æ–‡ä»¶")

    list_detail = f"æ¨¡å¼={run_mode}ï¼Œå€™é€‰={len(files_to_publish)}"
    log_step_ok(step, list_detail)
    set_status(step, "æˆåŠŸ", list_detail)

    # Step 4: publish
    step = 4
    log_step_start(step)
    SUCCESS_BATCH_SIZE_SMALL = 5
    SUCCESS_REST_SECONDS_SMALL = 3
    SUCCESS_BATCH_SIZE_LARGE = 20
    SUCCESS_REST_SECONDS_LARGE = 10

    success_count = 0
    skipped_count = 0
    failed_count = 0
    missing_count = 0

    daily_limit_reached = False
    processed = 0

    for idx, md_file in enumerate(files_to_publish, 1):
        if not os.path.exists(md_file):
            logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: '{md_file}'")
            failed_count += 1
            missing_count += 1
            continue

        logger.info(f"[{idx}/{len(files_to_publish)}] å¤„ç†æ–‡ä»¶: {md_file}")
        post_title = os.path.basename(md_file).replace('.md', '')
        post_content = get_file_content(md_file)

        try:
            result = post_to_cnblogs(post_title, post_content)
        except DailyLimitReached as e:
            logger.error(f"âŒ æ£€æµ‹åˆ°åšå®¢å›­å½“æ—¥å‘å¸ƒé¢åº¦å·²ç”¨å°½ï¼Œåœæ­¢æœ¬æ¬¡åŒæ­¥ï¼š{e}")
            processed = idx - 1
            daily_limit_reached = True
            break

        if result in {"created", "updated"}:
            success_count += 1
            if success_count % SUCCESS_BATCH_SIZE_SMALL == 0:
                logger.info(f"â³ å·²å¤„ç† {success_count} ç¯‡ï¼Œä¼‘æ¯ {SUCCESS_REST_SECONDS_SMALL}s...")
                time.sleep(SUCCESS_REST_SECONDS_SMALL)
                logger.info("âœ… ç»§ç»­åŒæ­¥...")

            if success_count % SUCCESS_BATCH_SIZE_LARGE == 0:
                logger.info(f"â³ å·²å¤„ç† {success_count} ç¯‡ï¼Œä¼‘æ¯ {SUCCESS_REST_SECONDS_LARGE}s...")
                time.sleep(SUCCESS_REST_SECONDS_LARGE)
                logger.info("âœ… ç»§ç»­åŒæ­¥...")
        elif result == "skipped":
            skipped_count += 1
        else:
            failed_count += 1
        processed = idx

    total = len(files_to_publish)
    if daily_limit_reached:
        step4_detail = (
            f"å› å½“æ—¥å‘å¸ƒé¢åº¦ç”¨å°½å·²åœæ­¢ï¼›æˆåŠŸ={success_count}ï¼Œè·³è¿‡={skipped_count}ï¼Œå¤±è´¥={failed_count}ï¼Œå·²å¤„ç†={processed}/{total}"
        )
    else:
        step4_detail = (
            f"æˆåŠŸ={success_count}ï¼Œè·³è¿‡={skipped_count}ï¼Œå¤±è´¥={failed_count}ï¼Œæ€»è®¡={total}"
        )
    if missing_count:
        step4_detail += f"ï¼Œç¼ºå¤±={missing_count}"
    log_step_ok(step, step4_detail)
    step4_status = "æˆåŠŸ" if (failed_count == 0 and not daily_limit_reached) else "éƒ¨åˆ†å¤±è´¥"
    set_status(step, step4_status, step4_detail)

    print_summary()
