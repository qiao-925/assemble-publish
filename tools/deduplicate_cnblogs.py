# deduplicate_cnblogs.py
# -*- coding: utf-8 -*-

import sys
import time
import xmlrpc.client
from collections import defaultdict
from datetime import datetime
from pathlib import Path

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥å…±äº«æ¨¡å—
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "src"))

from dotenv import load_dotenv
from assemble_publish.common import (
    logger,
    env_str,
    get_sync_record_path,
    load_sync_record,
    save_sync_record,
    get_blog_id,
)

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

# --- é…ç½®ä¿¡æ¯ ---
RPC_URL = env_str("CNBLOGS_RPC_URL")
USERNAME = env_str("CNBLOGS_USERNAME")
TOKEN = env_str("CNBLOGS_TOKEN")
BLOG_ID = None  # è‡ªåŠ¨è·å–

# åŒæ­¥è®°å½•æ–‡ä»¶è·¯å¾„
REPO_ROOT = Path.cwd().resolve()
SYNC_RECORD_FILE = get_sync_record_path(REPO_ROOT)

# --- é…ç½®é€‰é¡¹ ---
KEEP_LATEST = True
DRY_RUN = False
SHOW_DETAILS = False
DELETE_DELAY = 0
MAX_ROUNDS = 50


def normalize_title(title):
    """æ ‡å‡†åŒ–æ ‡é¢˜ï¼Œç”¨äºåŒ¹é…ï¼ˆå»é™¤é¦–å°¾ç©ºæ ¼ï¼‰"""
    return title.strip() if title else ""


def get_all_posts(server, max_posts=300):
    """è·å–æ‰€æœ‰æ–‡ç« ï¼ˆgetRecentPosts API æé™æ˜¯ 300 ç¯‡ï¼Œä¸æ”¯æŒåˆ†é¡µï¼‰"""
    all_posts = []
    all_post_ids = set()

    request_count = min(max_posts, 300)
    logger.info(f"ğŸ“¥ å¼€å§‹è·å–æ–‡ç« åˆ—è¡¨ï¼ˆè¯·æ±‚ {request_count} ç¯‡ï¼ŒAPI æé™æ˜¯ 300 ç¯‡ï¼‰...")

    try:
        posts = server.metaWeblog.getRecentPosts(BLOG_ID, USERNAME, TOKEN, request_count)

        if posts:
            for post in posts:
                post_id = post.get('postid')
                if post_id and post_id not in all_post_ids:
                    all_posts.append(post)
                    all_post_ids.add(post_id)

            logger.info(f"  âœ“ API è¿”å› {len(posts)} ç¯‡æ–‡ç« ")
            logger.info(f"  âœ“ å»é‡åå¾—åˆ° {len(all_posts)} ç¯‡ä¸é‡å¤æ–‡ç« ")

            if len(posts) == 300:
                logger.warning("  âš ï¸ æ³¨æ„ï¼šè¿”å›äº† 300 ç¯‡æ–‡ç« ï¼ˆAPI æé™ï¼‰ï¼Œå¯èƒ½è¿˜æœ‰æ›´æ—©çš„æ–‡ç« æœªè·å–")
        else:
            logger.info("  â„¹ï¸ æœªè·å–åˆ°ä»»ä½•æ–‡ç« ")

    except Exception as e:
        logger.error(f"è·å–æ–‡ç« æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    logger.info(f"âœ… å…±è·å– {len(all_posts)} ç¯‡ä¸é‡å¤æ–‡ç« ")
    return all_posts


def find_duplicates(posts):
    """æ‰¾å‡ºé‡å¤çš„æ–‡ç« ï¼ŒæŒ‰æ ‡é¢˜åˆ†ç»„"""
    title_groups = defaultdict(list)

    for post in posts:
        title = normalize_title(post.get('title', ''))
        if title:
            title_groups[title].append(post)

    return {title: posts_list for title, posts_list in title_groups.items() if len(posts_list) > 1}


def parse_date(date_value):
    """è§£ææ—¥æœŸå€¼ï¼Œè¿”å› datetime å¯¹è±¡ç”¨äºæ¯”è¾ƒ"""
    try:
        if isinstance(date_value, datetime):
            return date_value
        if isinstance(date_value, str):
            for fmt in ['%Y%m%dT%H:%M:%S', '%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%S%z']:
                try:
                    return datetime.strptime(date_value, fmt)
                except:
                    continue
        return datetime.fromisoformat(str(date_value)) if hasattr(datetime, 'fromisoformat') else datetime.now()
    except:
        return datetime(1970, 1, 1)


def format_date(date_value):
    """æ ¼å¼åŒ–æ—¥æœŸå­—ç¬¦ä¸²ç”¨äºæ˜¾ç¤º"""
    try:
        dt = parse_date(date_value)
        return dt.strftime('%Y-%m-%d %H:%M:%S')
    except:
        return str(date_value)


def delete_post(server, post_id, title):
    """åˆ é™¤æŒ‡å®šæ–‡ç« ï¼ˆä½¿ç”¨ blogger.deletePostï¼‰"""
    try:
        logger.debug(f"è°ƒç”¨åˆ é™¤æ¥å£: blogger.deletePost, postid='{post_id}'")
        result = server.blogger.deletePost('', post_id, USERNAME, TOKEN, True)

        if result is True or result == True:
            logger.info(f"      âœ… åˆ é™¤æˆåŠŸ")
            return True
        else:
            logger.error(f"      âŒ æ¥å£è¿”å› Falseï¼Œåˆ é™¤å¤±è´¥")
            return False

    except Exception as e:
        logger.error(f"      âŒ æ¥å£è°ƒç”¨å¤±è´¥: {type(e).__name__}: {e}")
        return False


def deduplicate_one_round(server):
    """æ‰§è¡Œä¸€è½®å»é‡ï¼Œè¿”å›æ˜¯å¦è¿˜æœ‰é‡å¤æ–‡ç« """
    global BLOG_ID

    sync_record = load_sync_record(SYNC_RECORD_FILE)
    updated_record = False

    all_posts = get_all_posts(server, max_posts=300)

    if not all_posts:
        logger.info("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ã€‚")
        return False

    duplicates = find_duplicates(all_posts)

    total_posts = len(all_posts)
    duplicate_titles_count = len(duplicates)
    duplicate_posts_count = sum(len(posts) for posts in duplicates.values())
    unique_posts_count = total_posts - duplicate_posts_count + duplicate_titles_count

    logger.info("=" * 80)
    logger.info("ğŸ“Š æ–‡ç« ç»Ÿè®¡ä¿¡æ¯")
    logger.info("=" * 80)
    logger.info(f"æ€»æ–‡ç« æ•°: {total_posts} ç¯‡")
    logger.info(f"ä¸é‡å¤æ–‡ç« : {unique_posts_count} ç¯‡")
    logger.info(f"é‡å¤æ ‡é¢˜æ•°: {duplicate_titles_count} ä¸ª")
    logger.info(f"é‡å¤æ–‡ç« æ€»æ•°: {duplicate_posts_count} ç¯‡")
    logger.info(f"å°†åˆ é™¤æ–‡ç« æ•°: {duplicate_posts_count - duplicate_titles_count} ç¯‡")

    if not duplicates:
        logger.info("âœ… æ²¡æœ‰å‘ç°é‡å¤æ–‡ç« ï¼")
        return False

    logger.info("=" * 80)
    logger.info("ğŸ“‹ é‡å¤æ ‡é¢˜è¯¦ç»†åˆ—è¡¨ï¼ˆæŒ‰é‡å¤æ•°é‡æ’åºï¼‰")
    logger.info("=" * 80)

    sorted_duplicates = sorted(duplicates.items(), key=lambda x: len(x[1]), reverse=True)

    for idx, (title, posts_list) in enumerate(sorted_duplicates, 1):
        logger.info(f"{idx:3d}. [{len(posts_list)} ç¯‡é‡å¤] {title}")
        show_count = min(5, len(posts_list))
        post_ids = [str(post.get('postid', 'N/A')) for post in posts_list[:show_count]]
        logger.info(f"     æ–‡ç« IDç¤ºä¾‹: {', '.join(post_ids)}")

    logger.info("=" * 80)
    logger.info("ğŸ” å¼€å§‹å¤„ç†é‡å¤æ–‡ç« ...")
    logger.info("=" * 80)

    total_to_delete = 0
    total_kept = 0
    total_deleted = 0
    total_failed = 0

    for title, posts_list in duplicates.items():
        logger.info(f"ğŸ“„ æ ‡é¢˜: {title}")
        logger.info(f"   é‡å¤æ•°é‡: {len(posts_list)} ç¯‡")

        try:
            posts_list.sort(key=lambda p: parse_date(p.get('dateCreated', p.get('pubDate', ''))), reverse=KEEP_LATEST)
        except:
            try:
                posts_list.sort(key=lambda p: int(p.get('postid', 0)), reverse=KEEP_LATEST)
            except:
                pass

        keep_post = posts_list[0]
        delete_posts = posts_list[1:]

        logger.info(f"   âœ“ ä¿ç•™: Post ID {keep_post.get('postid')} (åˆ›å»ºæ—¶é—´: {format_date(keep_post.get('dateCreated', keep_post.get('pubDate', 'N/A')))})")

        if sync_record is not None and not DRY_RUN:
            keep_id = keep_post.get('postid')
            if keep_id is not None:
                sync_record[title] = keep_id
                updated_record = True

        for post in delete_posts:
            post_id = post.get('postid')
            post_date = format_date(post.get('dateCreated', post.get('pubDate', 'N/A')))

            if DRY_RUN:
                logger.info(f"   ğŸ—‘ï¸  [æ¨¡æ‹Ÿ] å°†åˆ é™¤: Post ID {post_id} (åˆ›å»ºæ—¶é—´: {post_date})")
                total_to_delete += 1
            else:
                logger.info(f"   ğŸ—‘ï¸  æ­£åœ¨åˆ é™¤: Post ID {post_id} (åˆ›å»ºæ—¶é—´: {post_date})")
                success = delete_post(server, post_id, title)
                if success:
                    total_deleted += 1
                else:
                    total_failed += 1
                total_to_delete += 1
                if DELETE_DELAY > 0:
                    time.sleep(DELETE_DELAY)

        total_kept += 1

    logger.info("=" * 60)
    if DRY_RUN:
        logger.info(f"ğŸ“Š [æ¨¡æ‹Ÿæ¨¡å¼] ç»Ÿè®¡:")
        logger.info(f"   - å°†ä¿ç•™: {total_kept} ç¯‡æ–‡ç« ï¼ˆæ¯ç»„ä¿ç•™1ç¯‡æœ€æ–°çš„ï¼‰")
        logger.info(f"   - å°†åˆ é™¤: {total_to_delete} ç¯‡é‡å¤æ–‡ç« ï¼ˆæ—§çš„ï¼‰")
        logger.info("ğŸ’¡ æç¤º: å°† DRY_RUN è®¾ç½®ä¸º False åé‡æ–°è¿è¡Œä»¥å®é™…æ‰§è¡Œåˆ é™¤")
    else:
        logger.info(f"ğŸ“Š ç»Ÿè®¡:")
        logger.info(f"   - å·²ä¿ç•™: {total_kept} ç¯‡æ–‡ç« ï¼ˆæ¯ç»„ä¿ç•™1ç¯‡æœ€æ–°çš„ï¼‰")
        logger.info(f"   - å·²åˆ é™¤: {total_deleted} ç¯‡é‡å¤æ–‡ç« ï¼ˆæ—§çš„ï¼‰")
        if total_failed > 0:
            logger.warning(f"   - åˆ é™¤å¤±è´¥: {total_failed} ç¯‡")
    logger.info("=" * 60)

    if sync_record is not None:
        if DRY_RUN:
            logger.info(f"â„¹ï¸ DRY_RUN=trueï¼šæœªæ›´æ–°å‘å¸ƒè®°å½•æ–‡ä»¶: {SYNC_RECORD_FILE}")
        elif updated_record:
            save_sync_record(SYNC_RECORD_FILE, sync_record)

    return True


def deduplicate_posts():
    """ä¸»å»é‡é€»è¾‘ï¼ˆè¿­ä»£æ¨¡å¼ï¼šç›´åˆ°æ²¡æœ‰é‡å¤æ–‡ç« ï¼‰"""
    global BLOG_ID

    missing_vars = []
    if not RPC_URL:
        missing_vars.append("CNBLOGS_RPC_URL")
    if not USERNAME:
        missing_vars.append("CNBLOGS_USERNAME")
    if not TOKEN:
        missing_vars.append("CNBLOGS_TOKEN")

    if missing_vars:
        logger.error("âŒ é”™è¯¯ï¼šä»¥ä¸‹ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼š")
        for var in missing_vars:
            logger.error(f"   - {var}")
        logger.error("ğŸ’¡ è¯·åˆ›å»º .env æ–‡ä»¶å¹¶è®¾ç½®è¿™äº›å˜é‡ï¼Œæˆ–é€šè¿‡ç¯å¢ƒå˜é‡ç›´æ¥è®¾ç½®ã€‚")
        sys.exit(1)

    try:
        server = xmlrpc.client.ServerProxy(RPC_URL)

        if not BLOG_ID:
            BLOG_ID = get_blog_id(server, USERNAME, TOKEN)
            if not BLOG_ID:
                logger.error("âŒ æ— æ³•è·å–åšå®¢IDï¼Œé€€å‡ºã€‚")
                sys.exit(1)

        round_num = 1
        max_rounds = MAX_ROUNDS

        while round_num <= max_rounds:
            logger.info("=" * 80)
            logger.info(f"ğŸ”„ ç¬¬ {round_num} è½®å»é‡")
            logger.info("=" * 80)

            has_duplicates = deduplicate_one_round(server)

            if not has_duplicates:
                logger.info("=" * 80)
                logger.info(f"âœ… å®Œæˆï¼ç»è¿‡ {round_num} è½®å»é‡ï¼Œå·²æ— é‡å¤æ–‡ç« ")
                logger.info("=" * 80)
                break

            round_num += 1
            if round_num <= max_rounds:
                logger.info("â³ ç­‰å¾… 2 ç§’åå¼€å§‹ä¸‹ä¸€è½®...")
                time.sleep(2)

        if round_num > max_rounds:
            logger.warning("=" * 80)
            logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§è½®æ•°é™åˆ¶ ({max_rounds} è½®ï¼‰ï¼Œåœæ­¢è¿­ä»£")
            logger.warning("=" * 80)

    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    logger.info("ğŸš€ åšå®¢å›­æ–‡ç« å»é‡å·¥å…·")
    logger.info(f"   æ¨¡å¼: {'æ¨¡æ‹Ÿè¿è¡Œ' if DRY_RUN else 'å®é™…åˆ é™¤'}")
    logger.info(f"   ç­–ç•¥: {'ä¿ç•™æœ€æ–°' if KEEP_LATEST else 'ä¿ç•™æœ€æ—©'}")
    deduplicate_posts()
